{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import gym\n",
    "import pickle, os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement:\n",
    "There are four designated locations in the grid world indicated by R(ed), G(reen), Y(ellow), and B(lue). When the episode starts, the taxi starts off at a random square and the passenger is at a random location. The taxi drives to the passenger’s location, picks up the passenger, drives to the passenger’s destination (another one of the four specified locations), and then drops off the passenger. Once the passenger is dropped off, the episode ends."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actions\n",
    "There are 6 discrete deterministic actions:\n",
    "\n",
    "0: move south\n",
    "\n",
    "1: move north\n",
    "\n",
    "2: move east\n",
    "\n",
    "3: move west\n",
    "\n",
    "4: pickup passenger\n",
    "\n",
    "5: drop off passenger\n",
    "\n",
    "Observations\n",
    "There are 500 discrete states since there are 25 taxi positions, 5 possible locations of the passenger (including the case when the passenger is in the taxi), and 4 destination locations.\n",
    "\n",
    "Note that there are 400 states that can actually be reached during an episode. The missing states correspond to situations in which the passenger is at the same location as their destination, as this typically signals the end of an episode. Four additional states can be observed right after a successful episodes, when both the passenger and the taxi are at the destination. This gives a total of 404 reachable discrete states.\n",
    "\n",
    "Each state space is represented by the tuple: (taxi_row, taxi_col, passenger_location, destination)\n",
    "\n",
    "An observation is an integer that encodes the corresponding state. The state tuple can then be decoded with the “decode” method.\n",
    "\n",
    "Passenger locations:\n",
    "\n",
    "0: R(ed)\n",
    "\n",
    "1: G(reen)\n",
    "\n",
    "2: Y(ellow)\n",
    "\n",
    "3: B(lue)\n",
    "\n",
    "4: in taxi\n",
    "\n",
    "Destinations:\n",
    "\n",
    "0: R(ed)\n",
    "\n",
    "1: G(reen)\n",
    "\n",
    "2: Y(ellow)\n",
    "\n",
    "3: B(lue)\n",
    "\n",
    "#### Source : https://www.gymlibrary.dev/environments/toy_text/taxi/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space Discrete(6)\n",
      "State Space Discrete(500)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Taxi-v3\", render_mode=\"human\").env\n",
    "\n",
    "env.reset()\n",
    "env.render()\n",
    "\n",
    "print(\"Action Space {}\".format(env.action_space))\n",
    "\n",
    "print(\"State Space {}\".format(env.observation_space))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem and Solution\n",
    "\n",
    "The SARSA algorithm is a reinforcement learning (RL) algorithm that is often used to solve the Taxi problem. The Taxi problem is a classic RL problem where an agent (a taxi) needs to navigate a gridworld to pick up passengers and drop them off at their specified locations. The goal of the agent is to learn an optimal policy that maximizes its total reward.\n",
    "\n",
    "Here's an overview of how the SARSA algorithm works:\n",
    "\n",
    "State and Action Representation:\n",
    "\n",
    "The gridworld is divided into discrete states, representing the different configurations of the taxi, passengers, and destinations.\n",
    "Actions represent the possible moves the taxi can make: move north, south, east, or west, or pick up or drop off a passenger.\n",
    "Q-Table Initialization:\n",
    "\n",
    "The algorithm initializes a Q-table, which is a table that maps state-action pairs to their corresponding values.\n",
    "The Q-table is initially set to small random values or zeros.\n",
    "Exploration vs. Exploitation:\n",
    "\n",
    "The agent follows an exploration-exploitation strategy to balance between exploring new actions and exploiting the knowledge gained so far.\n",
    "During exploration, the agent takes random actions with a certain probability (epsilon-greedy exploration).\n",
    "During exploitation, the agent selects actions based on the current estimates in the Q-table.\n",
    "Action Selection:\n",
    "\n",
    "Based on the current state, the agent selects an action using the epsilon-greedy strategy.\n",
    "With probability (1-epsilon), the agent selects the action with the highest Q-value for that state (exploitation).\n",
    "With probability epsilon, the agent selects a random action (exploration).\n",
    "Action Execution and Reward:\n",
    "\n",
    "The agent executes the selected action in the environment and observes the resulting state and the immediate reward.\n",
    "The state transitions to a new state based on the action taken.\n",
    "Update Q-Table:\n",
    "\n",
    "The agent updates the Q-value of the previous state-action pair based on the observed reward and the Q-value of the new state-action pair.\n",
    "The update rule in SARSA is as follows:\n",
    "Q(s, a) = Q(s, a) + alpha * (r + gamma * Q(s', a') - Q(s, a))\n",
    "Q(s, a) is the Q-value of state-action pair (s, a)\n",
    "alpha is the learning rate, controlling the weight given to the new information (0 < alpha <= 1)\n",
    "r is the observed reward for taking action a in state s\n",
    "gamma is the discount factor, balancing the importance of immediate and future rewards (0 < gamma <= 1)\n",
    "Q(s', a') is the Q-value of the new state-action pair after taking action a in state s\n",
    "Repeat:\n",
    "\n",
    "Steps 4 to 6 are repeated until the agent reaches the terminal state (a passenger is dropped off).\n",
    "\n",
    "The agent updates the Q-values after each action taken, gradually improving its policy.\n",
    "By repeatedly updating the Q-values based on the observed rewards and using the epsilon-greedy strategy to balance exploration and exploitation, the SARSA algorithm allows the agent to learn an optimal policy for navigating the Taxi problem.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 223, -1, False)],\n",
       " 1: [(1.0, 23, -1, False)],\n",
       " 2: [(1.0, 123, -1, False)],\n",
       " 3: [(1.0, 103, -1, False)],\n",
       " 4: [(1.0, 123, -10, False)],\n",
       " 5: [(1.0, 123, -10, False)]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P[123]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining parameters\n",
    "epsilon = 1 # Total exploration and no exploitation\n",
    "alpha = 0.3\n",
    "gamma = 0.95\n",
    "\n",
    "# Training parameters\n",
    "max_episodes = 10  # number of episodes to use for training\n",
    "max_steps = 100   # maximum number of steps per episode\n",
    "\n",
    "#Initializing the Q-table 500x6\n",
    "Q = np.zeros((env.observation_space.n, env.action_space.n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to choose the next action\n",
    "def choose_action(state):\n",
    "\n",
    "    action=0\n",
    "\n",
    "    if np.random.uniform(0, 1) < epsilon:\n",
    "\n",
    "        action = env.action_space.sample()   # explore\n",
    "\n",
    "    else:\n",
    "        \n",
    "        action = np.argmax(Q[state, :])      # exploit\n",
    "        \n",
    "    return action\n",
    "\n",
    "# Function to update the Q-value\n",
    "def update(state, state2, reward, action, action2):\n",
    "    \n",
    "    predict = Q[state, action]\n",
    "    target = reward + gamma * Q[state2, action2]\n",
    "    \n",
    "    Q[state, action] = Q[state, action] + alpha * (target - predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  0\n",
      "Goal reached:  0\n",
      "Score:  0\n",
      " \n",
      "Episode:  1\n",
      "Goal reached:  0\n",
      "Score:  -406\n",
      " \n",
      "Episode:  2\n",
      "Goal reached:  0\n",
      "Score:  -424\n",
      " \n",
      "Episode:  3\n",
      "Goal reached:  0\n",
      "Score:  -334\n",
      " \n",
      "Episode:  4\n",
      "Goal reached:  0\n",
      "Score:  -415\n",
      " \n",
      "Episode:  5\n",
      "Goal reached:  0\n",
      "Score:  -370\n",
      " \n",
      "Episode:  6\n",
      "Goal reached:  0\n",
      "Score:  -316\n",
      " \n",
      "Episode:  7\n",
      "Goal reached:  0\n",
      "Score:  -334\n",
      " \n",
      "Episode:  8\n",
      "Goal reached:  0\n",
      "Score:  -334\n",
      " \n",
      "Episode:  9\n",
      "Goal reached:  0\n",
      "Score:  -316\n",
      " \n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Initializing parameters\n",
    "score = 0\n",
    "goal = 0 \n",
    "episode = 0\n",
    "\n",
    "\n",
    "# Starting the SARSA learning\n",
    "for episode in range(max_episodes):\n",
    "\n",
    "    # Print each episode\n",
    "    print(\"Episode: \", episode)\n",
    "    print(\"Goal reached: \", goal)\n",
    "    print(\"Score: \", score)\n",
    "    print(\" \")\n",
    "    \n",
    "    state1 = env.reset()\n",
    "    state1 = state1[0]\n",
    "    action1 = choose_action(state1)\n",
    "    \n",
    "    # Reset parameters\n",
    "    s = 0\n",
    "    score = 0\n",
    "    done = False\n",
    "    \n",
    "    # Decreasing epsilon\n",
    "    epsilon -= 0.005\n",
    "    \n",
    "    if epsilon < 0.05:\n",
    "        epsilon = 0.05    # epsilon min: total exploitation (95%)\n",
    " \n",
    "\n",
    "    for s in range(max_steps):\n",
    "        \n",
    "        # Rendering\n",
    "        env.render()\n",
    "         \n",
    "        # Get the next state\n",
    "        state2, reward, done, truncate, info = env.step(action1)\n",
    " \n",
    "        # Choose the next action\n",
    "        action2 = choose_action(state2)\n",
    "         \n",
    "        # Update the Q-value\n",
    "        update(state1, state2, reward, action1, action2)\n",
    " \n",
    "        state1 = state2\n",
    "        action1 = action2\n",
    "         \n",
    "        # Updating parameters\n",
    "        s += 1\n",
    "        score += reward\n",
    "        \n",
    "        if reward==20:\n",
    "            goal +=1\n",
    "         \n",
    "        #If at the end of learning process\n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "The program needs to run for a significant number of episodes, possibly ranging in the hundreds or thousands, which can take many hours to complete. Throughout the learning process, the agent initially focuses on exploration, leading to numerous mistakes as it explores various scenarios.\n",
    "\n",
    "In the beginning, during the first episode, the agent engages in pure exploration, resulting in a failure to reach the goal and a negative score of -415.\n",
    "\n",
    "After around 100 episodes, the balance between exploration and exploitation becomes equal, as the value of epsilon decreases. Although the agent is still learning, it can occasionally achieve some goals, although with a considerable negative score of -325.\n",
    "\n",
    "Continuing further to about 200 episodes, the agent demonstrates a growing capability to reach goals more frequently, even though it still makes mistakes. At this point, the agent shifts into pure exploitation, having learned enough to make informed decisions. The score improves to -118.\n",
    "\n",
    "Over the subsequent 100 episodes, from 201 to 300, the agent's success in reaching goals becomes more consistent, as evidenced by reaching the goal 92 times. The score further improves to -31.\n",
    "\n",
    "After 400 episodes and beyond, the agent has gained extensive knowledge about how to interact optimally in each situation, consistently making the best decisions and taking the shortest paths to reach the goals. It becomes rare for the agent to fail in reaching the goal.\n",
    "\n",
    "For example, in the 401st episode, the agent successfully reached the goal 182 times, resulting in a score of -2. As the number of episodes increases, the agent's performance continues to improve significantly, with 3779 successful goal completions out of 4000 episodes and a positive score of 10."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
