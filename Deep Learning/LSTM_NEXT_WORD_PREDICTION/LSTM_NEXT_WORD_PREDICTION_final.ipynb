{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# WARNING\n",
        "<h1> <i> <b> If you are running  this notebook in Local env then it may crash due to low computing power, the same may happen if running in google colab without GPU</b></i></h1>\n",
        "\n",
        "\n",
        "<h2><i><b>\n",
        "Min requirements:\n",
        "16 GB RAM\n",
        "16 GB GPU RAM\n",
        "</b></i></h2>"
      ],
      "metadata": {
        "id": "AtQAA96e7bZk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "utMxBcFcBTvE"
      },
      "outputs": [],
      "source": [
        "# Dataset : Text file version of the book from below\n",
        "# https://www.gutenberg.org/browse/scores/top\n",
        "# I have Prepared a dataset by mixing the books from above website :)\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import Embedding , LSTM, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import pickle\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This below cell takes a good amount of time and computing power\n",
        "# so i am uploading both the data_lstm.txt file which has raw data\n",
        "# and data_redu.text which has the preprocessed data, i.e. output from this cell"
      ],
      "metadata": {
        "id": "DRf2X7st8G--"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQyMJ7iRDUqt"
      },
      "outputs": [],
      "source": [
        "file = open(\"data_lstm.txt\",'r',encoding='utf8')\n",
        "\n",
        "lines = list()\n",
        "for i in file:\n",
        "  lines.append(i)\n",
        "\n",
        "data = \"\"\n",
        "for i in lines:\n",
        "  data = ' '.join(lines)\n",
        "\n",
        "data = data.replace('\\n','').replace('\\r','').replace('\\ufeff','').replace('\"','')\n",
        "\n",
        "data = data.split()\n",
        "data = ' '.join(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-xe4GiyPy2l"
      },
      "outputs": [],
      "source": [
        "# f = open(\"data_redu.txt\",'w')\n",
        "# f.write(data)\n",
        "# f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ATIy1KTEYNY",
        "outputId": "e0bbe0ec-72a3-434e-c04b-a9834c7eef0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "636719\n"
          ]
        }
      ],
      "source": [
        "f = open(\"data_redu.txt\",'r')\n",
        "data = f.read()\n",
        "f.close()\n",
        "# data[:819]\n",
        "print(len(data))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kN_mN46-Eibl",
        "outputId": "9acbeaa0-f0c7-450d-db2b-dc1c468c9223"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3365, 570, 161, 5, 23, 27, 2204, 2, 156, 23, 16, 6, 275, 80, 1]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([data])\n",
        "pickle.dump(tokenizer,open('token.pkl','wb'))\n",
        "# tokenizer = pickle.load(open('token.pkl','rb'))\n",
        "seq_data = tokenizer.texts_to_sequences([data])[0]\n",
        "seq_data[:15]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUBsyVhkPQfn",
        "outputId": "ab92f208-2f01-48a5-915c-75515743d6f9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "117578"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "len(seq_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rs41YrCHPbVC",
        "outputId": "c4d72b90-2f37-414c-9304-cfa25a467dc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8459\n"
          ]
        }
      ],
      "source": [
        "vocab_size = len(tokenizer.word_index)+1\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "7ehVDkCgPmn3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d04cafa-baef-490d-befd-ccbb7dd0be8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Len of seq are :  117575\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3365,  570,  161,    5],\n",
              "       [ 570,  161,    5,   23],\n",
              "       [ 161,    5,   23,   27],\n",
              "       [   5,   23,   27, 2204],\n",
              "       [  23,   27, 2204,    2],\n",
              "       [  27, 2204,    2,  156],\n",
              "       [2204,    2,  156,   23],\n",
              "       [   2,  156,   23,   16],\n",
              "       [ 156,   23,   16,    6],\n",
              "       [  23,   16,    6,  275]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "sequences = list()\n",
        "\n",
        "for i in range(3,len(seq_data)):\n",
        "  words = seq_data[i-3:i+1]\n",
        "  sequences.append(words)\n",
        "\n",
        "print(\"The Len of seq are : \",len(sequences))\n",
        "sequences = np.array(sequences)\n",
        "sequences[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "c8vE2eaZQJHf"
      },
      "outputs": [],
      "source": [
        "X = []\n",
        "y = []\n",
        "\n",
        "for i in sequences:\n",
        "  X.append(i[0:3])\n",
        "  y.append(i[3])\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6LnMuRuQcEh",
        "outputId": "0993c7d5-5c91-41d6-9628-0ec634f5ebb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data :  [[3365  570  161]\n",
            " [ 570  161    5]\n",
            " [ 161    5   23]\n",
            " [   5   23   27]\n",
            " [  23   27 2204]\n",
            " [  27 2204    2]\n",
            " [2204    2  156]\n",
            " [   2  156   23]\n",
            " [ 156   23   16]\n",
            " [  23   16    6]]\n",
            "Output :  [   5   23   27 2204    2  156   23   16    6  275]\n"
          ]
        }
      ],
      "source": [
        "print(\"Data : \", X[:10])\n",
        "print(\"Output : \", y[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJqcaft_QjVJ",
        "outputId": "a55202bb-17bc-4b7a-fc33-a433a9ed6aa4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 1., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "y = to_categorical(y,num_classes = vocab_size)\n",
        "y[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "6qhaUPukSZaT"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# import torchvision\n",
        "# def get_default_device():\n",
        "#     \"\"\"Picking GPU if available or else CPU\"\"\"\n",
        "#     if torch.cuda.is_available():\n",
        "#         return torch.device('cuda')\n",
        "#     else:\n",
        "#         return torch.device('cpu')\n",
        "# device = get_default_device()\n",
        "\n",
        "# def to_device(data, device):\n",
        "#     \"\"\"Move tensor(s) to chosen device\"\"\"\n",
        "#     if isinstance(data, (list,tuple,str)):\n",
        "#         return [to_device(x, device) for x in data]\n",
        "#     return data.to(device, non_blocking=True)\n",
        "\n",
        "# to_device([data],device)\n",
        "\n",
        "\n",
        "\n",
        "# Ignore This.\n",
        "# May need when GPU is not responding in colab or your device :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "FJFZniQ6Qwd8"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size,10,input_length=3))\n",
        "model.add(LSTM(1000,return_sequences=True))\n",
        "model.add(LSTM(1000))\n",
        "model.add(Dense(1000,activation=\"relu\"))\n",
        "model.add(Dense(vocab_size,activation=\"softmax\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0xH4T54Rcq9",
        "outputId": "6f5602c7-76dd-4818-8a03-106476b782cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 3, 10)             84590     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 3, 1000)           4044000   \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 1000)              8004000   \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1000)              1001000   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 8459)              8467459   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 21,601,049\n",
            "Trainable params: 21,601,049\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "Lhn2OsF0R_w4",
        "outputId": "3b28eb72-7963-4c55-fd43-a91d9fdce508"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from tensorflow import keras\n",
        "from keras.utils.vis_utils import plot_model\n",
        "keras.utils.plot_model(model,to_file='model.png',show_layer_names=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model training Will Take 30 to 45 mins with above mentioned specs"
      ],
      "metadata": {
        "id": "dS0HUFBb8SwU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "lCV6NoggSXfq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85c872ef-ab91-41d1-b68c-4e5f9eebc66f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1838/1838 [==============================] - ETA: 0s - loss: 6.4272\n",
            "Epoch 1: loss improved from inf to 6.42723, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 50s 22ms/step - loss: 6.4272\n",
            "Epoch 2/100\n",
            "1837/1838 [============================>.] - ETA: 0s - loss: 5.8408\n",
            "Epoch 2: loss improved from 6.42723 to 5.84090, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 28s 15ms/step - loss: 5.8409\n",
            "Epoch 3/100\n",
            "1838/1838 [==============================] - ETA: 0s - loss: 5.4816\n",
            "Epoch 3: loss improved from 5.84090 to 5.48161, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 29s 16ms/step - loss: 5.4816\n",
            "Epoch 4/100\n",
            "1838/1838 [==============================] - ETA: 0s - loss: 5.2042\n",
            "Epoch 4: loss improved from 5.48161 to 5.20416, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 29s 16ms/step - loss: 5.2042\n",
            "Epoch 5/100\n",
            "1838/1838 [==============================] - ETA: 0s - loss: 4.9608\n",
            "Epoch 5: loss improved from 5.20416 to 4.96077, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 28s 15ms/step - loss: 4.9608\n",
            "Epoch 6/100\n",
            "1838/1838 [==============================] - ETA: 0s - loss: 4.7244\n",
            "Epoch 6: loss improved from 4.96077 to 4.72440, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 28s 15ms/step - loss: 4.7244\n",
            "Epoch 7/100\n",
            "1838/1838 [==============================] - ETA: 0s - loss: 4.4945\n",
            "Epoch 7: loss improved from 4.72440 to 4.49453, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 29s 16ms/step - loss: 4.4945\n",
            "Epoch 8/100\n",
            "1837/1838 [============================>.] - ETA: 0s - loss: 4.2618\n",
            "Epoch 8: loss improved from 4.49453 to 4.26185, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 28s 15ms/step - loss: 4.2619\n",
            "Epoch 9/100\n",
            "1838/1838 [==============================] - ETA: 0s - loss: 4.0261\n",
            "Epoch 9: loss improved from 4.26185 to 4.02611, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 28s 15ms/step - loss: 4.0261\n",
            "Epoch 10/100\n",
            "1835/1838 [============================>.] - ETA: 0s - loss: 3.7784\n",
            "Epoch 10: loss improved from 4.02611 to 3.77843, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 28s 15ms/step - loss: 3.7784\n",
            "Epoch 11/100\n",
            "1836/1838 [============================>.] - ETA: 0s - loss: 3.5306\n",
            "Epoch 11: loss improved from 3.77843 to 3.53056, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 33s 18ms/step - loss: 3.5306\n",
            "Epoch 12/100\n",
            "1837/1838 [============================>.] - ETA: 0s - loss: 3.2827\n",
            "Epoch 12: loss improved from 3.53056 to 3.28268, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 28s 15ms/step - loss: 3.2827\n",
            "Epoch 13/100\n",
            "1838/1838 [==============================] - ETA: 0s - loss: 3.0373\n",
            "Epoch 13: loss improved from 3.28268 to 3.03727, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 28s 15ms/step - loss: 3.0373\n",
            "Epoch 14/100\n",
            "1838/1838 [==============================] - ETA: 0s - loss: 2.7873\n",
            "Epoch 14: loss improved from 3.03727 to 2.78726, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 28s 15ms/step - loss: 2.7873\n",
            "Epoch 15/100\n",
            "1838/1838 [==============================] - ETA: 0s - loss: 2.5411\n",
            "Epoch 15: loss improved from 2.78726 to 2.54109, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 28s 15ms/step - loss: 2.5411\n",
            "Epoch 16/100\n",
            "1836/1838 [============================>.] - ETA: 0s - loss: 2.2948\n",
            "Epoch 16: loss improved from 2.54109 to 2.29506, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 28s 15ms/step - loss: 2.2951\n",
            "Epoch 17/100\n",
            "1837/1838 [============================>.] - ETA: 0s - loss: 2.0554\n",
            "Epoch 17: loss improved from 2.29506 to 2.05544, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 31s 17ms/step - loss: 2.0554\n",
            "Epoch 18/100\n",
            "1837/1838 [============================>.] - ETA: 0s - loss: 1.8274\n",
            "Epoch 18: loss improved from 2.05544 to 1.82739, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 28s 15ms/step - loss: 1.8274\n",
            "Epoch 19/100\n",
            "1836/1838 [============================>.] - ETA: 0s - loss: 1.6212\n",
            "Epoch 19: loss improved from 1.82739 to 1.62140, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 28s 15ms/step - loss: 1.6214\n",
            "Epoch 20/100\n",
            "1838/1838 [==============================] - ETA: 0s - loss: 1.4394\n",
            "Epoch 20: loss improved from 1.62140 to 1.43937, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 28s 15ms/step - loss: 1.4394\n",
            "Epoch 21/100\n",
            "1838/1838 [==============================] - ETA: 0s - loss: 1.2857\n",
            "Epoch 21: loss improved from 1.43937 to 1.28565, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 28s 15ms/step - loss: 1.2857\n",
            "Epoch 22/100\n",
            "1837/1838 [============================>.] - ETA: 0s - loss: 1.1522\n",
            "Epoch 22: loss improved from 1.28565 to 1.15223, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 28s 15ms/step - loss: 1.1522\n",
            "Epoch 23/100\n",
            "1837/1838 [============================>.] - ETA: 0s - loss: 1.0496\n",
            "Epoch 23: loss improved from 1.15223 to 1.04965, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 33s 18ms/step - loss: 1.0496\n",
            "Epoch 24/100\n",
            "1837/1838 [============================>.] - ETA: 0s - loss: 0.9596\n",
            "Epoch 24: loss improved from 1.04965 to 0.95956, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 28s 15ms/step - loss: 0.9596\n",
            "Epoch 25/100\n",
            "1837/1838 [============================>.] - ETA: 0s - loss: 0.8948\n",
            "Epoch 25: loss improved from 0.95956 to 0.89478, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 28s 15ms/step - loss: 0.8948\n",
            "Epoch 26/100\n",
            "1838/1838 [==============================] - ETA: 0s - loss: 0.8335\n",
            "Epoch 26: loss improved from 0.89478 to 0.83351, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 28s 15ms/step - loss: 0.8335\n",
            "Epoch 27/100\n",
            "1835/1838 [============================>.] - ETA: 0s - loss: 0.7880\n",
            "Epoch 27: loss improved from 0.83351 to 0.78822, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 28s 15ms/step - loss: 0.7882\n",
            "Epoch 28/100\n",
            "1835/1838 [============================>.] - ETA: 0s - loss: 0.7557\n",
            "Epoch 28: loss improved from 0.78822 to 0.75597, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 32s 18ms/step - loss: 0.7560\n",
            "Epoch 29/100\n",
            "1835/1838 [============================>.] - ETA: 0s - loss: 0.7192\n",
            "Epoch 29: loss improved from 0.75597 to 0.71950, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 28s 15ms/step - loss: 0.7195\n",
            "Epoch 30/100\n",
            "1837/1838 [============================>.] - ETA: 0s - loss: 0.6919\n",
            "Epoch 30: loss improved from 0.71950 to 0.69191, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 28s 15ms/step - loss: 0.6919\n",
            "Epoch 31/100\n",
            "1837/1838 [============================>.] - ETA: 0s - loss: 0.6751\n",
            "Epoch 31: loss improved from 0.69191 to 0.67512, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 28s 15ms/step - loss: 0.6751\n",
            "Epoch 32/100\n",
            "1838/1838 [==============================] - ETA: 0s - loss: 0.6478\n",
            "Epoch 32: loss improved from 0.67512 to 0.64776, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 28s 15ms/step - loss: 0.6478\n",
            "Epoch 33/100\n",
            "1837/1838 [============================>.] - ETA: 0s - loss: 0.6340\n",
            "Epoch 33: loss improved from 0.64776 to 0.63400, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 28s 15ms/step - loss: 0.6340\n",
            "Epoch 34/100\n",
            "1838/1838 [==============================] - ETA: 0s - loss: 0.6174\n",
            "Epoch 34: loss improved from 0.63400 to 0.61743, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 28s 15ms/step - loss: 0.6174\n",
            "Epoch 35/100\n",
            "1838/1838 [==============================] - ETA: 0s - loss: 0.6059\n",
            "Epoch 35: loss improved from 0.61743 to 0.60586, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 28s 15ms/step - loss: 0.6059\n",
            "Epoch 36/100\n",
            "1838/1838 [==============================] - ETA: 0s - loss: 0.5893\n",
            "Epoch 36: loss improved from 0.60586 to 0.58934, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 28s 15ms/step - loss: 0.5893\n",
            "Epoch 37/100\n",
            "1836/1838 [============================>.] - ETA: 0s - loss: 0.5763\n",
            "Epoch 37: loss improved from 0.58934 to 0.57642, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 28s 15ms/step - loss: 0.5764\n",
            "Epoch 38/100\n",
            "1836/1838 [============================>.] - ETA: 0s - loss: 0.5660\n",
            "Epoch 38: loss improved from 0.57642 to 0.56599, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 32s 17ms/step - loss: 0.5660\n",
            "Epoch 39/100\n",
            "1837/1838 [============================>.] - ETA: 0s - loss: 0.5561\n",
            "Epoch 39: loss improved from 0.56599 to 0.55612, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 28s 15ms/step - loss: 0.5561\n",
            "Epoch 40/100\n",
            "1835/1838 [============================>.] - ETA: 0s - loss: 0.5489\n",
            "Epoch 40: loss improved from 0.55612 to 0.54927, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 28s 15ms/step - loss: 0.5493\n",
            "Epoch 41/100\n",
            "1836/1838 [============================>.] - ETA: 0s - loss: 0.5373\n",
            "Epoch 41: loss improved from 0.54927 to 0.53727, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 33s 18ms/step - loss: 0.5373\n",
            "Epoch 42/100\n",
            "1838/1838 [==============================] - ETA: 0s - loss: 0.5327\n",
            "Epoch 42: loss improved from 0.53727 to 0.53272, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 28s 15ms/step - loss: 0.5327\n",
            "Epoch 43/100\n",
            "1835/1838 [============================>.] - ETA: 0s - loss: 0.5212\n",
            "Epoch 43: loss improved from 0.53272 to 0.52140, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 28s 15ms/step - loss: 0.5214\n",
            "Epoch 44/100\n",
            "1835/1838 [============================>.] - ETA: 0s - loss: 0.5174\n",
            "Epoch 44: loss improved from 0.52140 to 0.51738, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 33s 18ms/step - loss: 0.5174\n",
            "Epoch 45/100\n",
            "1835/1838 [============================>.] - ETA: 0s - loss: 0.5086\n",
            "Epoch 45: loss improved from 0.51738 to 0.50861, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 32s 18ms/step - loss: 0.5086\n",
            "Epoch 46/100\n",
            "1838/1838 [==============================] - ETA: 0s - loss: 0.5017\n",
            "Epoch 46: loss improved from 0.50861 to 0.50171, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 30s 16ms/step - loss: 0.5017\n",
            "Epoch 47/100\n",
            "1836/1838 [============================>.] - ETA: 0s - loss: 0.4974\n",
            "Epoch 47: loss improved from 0.50171 to 0.49747, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 28s 15ms/step - loss: 0.4975\n",
            "Epoch 48/100\n",
            "1835/1838 [============================>.] - ETA: 0s - loss: 0.4930\n",
            "Epoch 48: loss improved from 0.49747 to 0.49329, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 28s 15ms/step - loss: 0.4933\n",
            "Epoch 49/100\n",
            "1837/1838 [============================>.] - ETA: 0s - loss: 0.4833\n",
            "Epoch 49: loss improved from 0.49329 to 0.48325, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 28s 15ms/step - loss: 0.4832\n",
            "Epoch 50/100\n",
            "1835/1838 [============================>.] - ETA: 0s - loss: 0.4803\n",
            "Epoch 50: loss improved from 0.48325 to 0.48045, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 28s 15ms/step - loss: 0.4804\n",
            "Epoch 51/100\n",
            "1837/1838 [============================>.] - ETA: 0s - loss: 0.4736\n",
            "Epoch 51: loss improved from 0.48045 to 0.47365, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 28s 15ms/step - loss: 0.4736\n",
            "Epoch 52/100\n",
            "1836/1838 [============================>.] - ETA: 0s - loss: 0.4742\n",
            "Epoch 52: loss did not improve from 0.47365\n",
            "1838/1838 [==============================] - 27s 15ms/step - loss: 0.4743\n",
            "Epoch 53/100\n",
            "1838/1838 [==============================] - ETA: 0s - loss: 0.4659\n",
            "Epoch 53: loss improved from 0.47365 to 0.46589, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 33s 18ms/step - loss: 0.4659\n",
            "Epoch 54/100\n",
            "1838/1838 [==============================] - ETA: 0s - loss: 0.4639\n",
            "Epoch 54: loss improved from 0.46589 to 0.46393, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 32s 18ms/step - loss: 0.4639\n",
            "Epoch 55/100\n",
            "1836/1838 [============================>.] - ETA: 0s - loss: 0.4565\n",
            "Epoch 55: loss improved from 0.46393 to 0.45650, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 28s 15ms/step - loss: 0.4565\n",
            "Epoch 56/100\n",
            "1837/1838 [============================>.] - ETA: 0s - loss: 0.4509\n",
            "Epoch 56: loss improved from 0.45650 to 0.45094, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 28s 15ms/step - loss: 0.4509\n",
            "Epoch 57/100\n",
            "1836/1838 [============================>.] - ETA: 0s - loss: 0.4522\n",
            "Epoch 57: loss did not improve from 0.45094\n",
            "1838/1838 [==============================] - 27s 15ms/step - loss: 0.4522\n",
            "Epoch 58/100\n",
            "1837/1838 [============================>.] - ETA: 0s - loss: 0.4481\n",
            "Epoch 58: loss improved from 0.45094 to 0.44806, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 33s 18ms/step - loss: 0.4481\n",
            "Epoch 59/100\n",
            "1836/1838 [============================>.] - ETA: 0s - loss: 0.4426\n",
            "Epoch 59: loss improved from 0.44806 to 0.44269, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 33s 18ms/step - loss: 0.4427\n",
            "Epoch 60/100\n",
            "1838/1838 [==============================] - ETA: 0s - loss: 0.4405\n",
            "Epoch 60: loss improved from 0.44269 to 0.44054, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 30s 16ms/step - loss: 0.4405\n",
            "Epoch 61/100\n",
            "1836/1838 [============================>.] - ETA: 0s - loss: 0.4373\n",
            "Epoch 61: loss improved from 0.44054 to 0.43720, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 28s 15ms/step - loss: 0.4372\n",
            "Epoch 62/100\n",
            "1837/1838 [============================>.] - ETA: 0s - loss: 0.4381\n",
            "Epoch 62: loss did not improve from 0.43720\n",
            "1838/1838 [==============================] - 27s 15ms/step - loss: 0.4381\n",
            "Epoch 63/100\n",
            "1835/1838 [============================>.] - ETA: 0s - loss: 0.4296\n",
            "Epoch 63: loss improved from 0.43720 to 0.42969, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 34s 18ms/step - loss: 0.4297\n",
            "Epoch 64/100\n",
            "1836/1838 [============================>.] - ETA: 0s - loss: 0.4273\n",
            "Epoch 64: loss improved from 0.42969 to 0.42729, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 28s 15ms/step - loss: 0.4273\n",
            "Epoch 65/100\n",
            "1838/1838 [==============================] - ETA: 0s - loss: 0.4255\n",
            "Epoch 65: loss improved from 0.42729 to 0.42554, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 28s 15ms/step - loss: 0.4255\n",
            "Epoch 66/100\n",
            "1835/1838 [============================>.] - ETA: 0s - loss: 0.4247\n",
            "Epoch 66: loss improved from 0.42554 to 0.42479, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 28s 15ms/step - loss: 0.4248\n",
            "Epoch 67/100\n",
            "1838/1838 [==============================] - ETA: 0s - loss: 0.4191\n",
            "Epoch 67: loss improved from 0.42479 to 0.41913, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 33s 18ms/step - loss: 0.4191\n",
            "Epoch 68/100\n",
            "1837/1838 [============================>.] - ETA: 0s - loss: 0.4209\n",
            "Epoch 68: loss did not improve from 0.41913\n",
            "1838/1838 [==============================] - 27s 15ms/step - loss: 0.4208\n",
            "Epoch 69/100\n",
            "1835/1838 [============================>.] - ETA: 0s - loss: 0.4149\n",
            "Epoch 69: loss improved from 0.41913 to 0.41503, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 33s 18ms/step - loss: 0.4150\n",
            "Epoch 70/100\n",
            "1837/1838 [============================>.] - ETA: 0s - loss: 0.4137\n",
            "Epoch 70: loss improved from 0.41503 to 0.41370, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 33s 18ms/step - loss: 0.4137\n",
            "Epoch 71/100\n",
            "1835/1838 [============================>.] - ETA: 0s - loss: 0.4138\n",
            "Epoch 71: loss did not improve from 0.41370\n",
            "1838/1838 [==============================] - 28s 15ms/step - loss: 0.4139\n",
            "Epoch 72/100\n",
            "1837/1838 [============================>.] - ETA: 0s - loss: 0.4102\n",
            "Epoch 72: loss improved from 0.41370 to 0.41023, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 33s 18ms/step - loss: 0.4102\n",
            "Epoch 73/100\n",
            "1838/1838 [==============================] - ETA: 0s - loss: 0.4088\n",
            "Epoch 73: loss improved from 0.41023 to 0.40876, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 29s 16ms/step - loss: 0.4088\n",
            "Epoch 74/100\n",
            "1838/1838 [==============================] - ETA: 0s - loss: 0.4046\n",
            "Epoch 74: loss improved from 0.40876 to 0.40461, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 30s 16ms/step - loss: 0.4046\n",
            "Epoch 75/100\n",
            "1837/1838 [============================>.] - ETA: 0s - loss: 0.4006\n",
            "Epoch 75: loss improved from 0.40461 to 0.40063, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 28s 15ms/step - loss: 0.4006\n",
            "Epoch 76/100\n",
            "1835/1838 [============================>.] - ETA: 0s - loss: 0.4037\n",
            "Epoch 76: loss did not improve from 0.40063\n",
            "1838/1838 [==============================] - 28s 15ms/step - loss: 0.4036\n",
            "Epoch 77/100\n",
            "1837/1838 [============================>.] - ETA: 0s - loss: 0.4031\n",
            "Epoch 77: loss did not improve from 0.40063\n",
            "1838/1838 [==============================] - 28s 15ms/step - loss: 0.4032\n",
            "Epoch 78/100\n",
            "1837/1838 [============================>.] - ETA: 0s - loss: 0.3989\n",
            "Epoch 78: loss improved from 0.40063 to 0.39889, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 36s 19ms/step - loss: 0.3989\n",
            "Epoch 79/100\n",
            "1838/1838 [==============================] - ETA: 0s - loss: 0.3967\n",
            "Epoch 79: loss improved from 0.39889 to 0.39668, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 32s 17ms/step - loss: 0.3967\n",
            "Epoch 80/100\n",
            "1836/1838 [============================>.] - ETA: 0s - loss: 0.3981\n",
            "Epoch 80: loss did not improve from 0.39668\n",
            "1838/1838 [==============================] - 27s 15ms/step - loss: 0.3980\n",
            "Epoch 81/100\n",
            "1838/1838 [==============================] - ETA: 0s - loss: 0.3919\n",
            "Epoch 81: loss improved from 0.39668 to 0.39188, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 32s 18ms/step - loss: 0.3919\n",
            "Epoch 82/100\n",
            "1836/1838 [============================>.] - ETA: 0s - loss: 0.3917\n",
            "Epoch 82: loss improved from 0.39188 to 0.39179, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 33s 18ms/step - loss: 0.3918\n",
            "Epoch 83/100\n",
            "1835/1838 [============================>.] - ETA: 0s - loss: 0.3932\n",
            "Epoch 83: loss did not improve from 0.39179\n",
            "1838/1838 [==============================] - 27s 15ms/step - loss: 0.3932\n",
            "Epoch 84/100\n",
            "1838/1838 [==============================] - ETA: 0s - loss: 0.3899\n",
            "Epoch 84: loss improved from 0.39179 to 0.38985, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 33s 18ms/step - loss: 0.3899\n",
            "Epoch 85/100\n",
            "1837/1838 [============================>.] - ETA: 0s - loss: 0.3877\n",
            "Epoch 85: loss improved from 0.38985 to 0.38770, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 32s 18ms/step - loss: 0.3877\n",
            "Epoch 86/100\n",
            "1838/1838 [==============================] - ETA: 0s - loss: 0.3891\n",
            "Epoch 86: loss did not improve from 0.38770\n",
            "1838/1838 [==============================] - 28s 15ms/step - loss: 0.3891\n",
            "Epoch 87/100\n",
            "1837/1838 [============================>.] - ETA: 0s - loss: 0.3830\n",
            "Epoch 87: loss improved from 0.38770 to 0.38311, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 32s 18ms/step - loss: 0.3831\n",
            "Epoch 88/100\n",
            "1835/1838 [============================>.] - ETA: 0s - loss: 0.3846\n",
            "Epoch 88: loss did not improve from 0.38311\n",
            "1838/1838 [==============================] - 28s 15ms/step - loss: 0.3849\n",
            "Epoch 89/100\n",
            "1838/1838 [==============================] - ETA: 0s - loss: 0.3857\n",
            "Epoch 89: loss did not improve from 0.38311\n",
            "1838/1838 [==============================] - 27s 15ms/step - loss: 0.3857\n",
            "Epoch 90/100\n",
            "1837/1838 [============================>.] - ETA: 0s - loss: 0.3793\n",
            "Epoch 90: loss improved from 0.38311 to 0.37932, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 34s 19ms/step - loss: 0.3793\n",
            "Epoch 91/100\n",
            "1836/1838 [============================>.] - ETA: 0s - loss: 0.3804\n",
            "Epoch 91: loss did not improve from 0.37932\n",
            "1838/1838 [==============================] - 27s 15ms/step - loss: 0.3803\n",
            "Epoch 92/100\n",
            "1837/1838 [============================>.] - ETA: 0s - loss: 0.3813\n",
            "Epoch 92: loss did not improve from 0.37932\n",
            "1838/1838 [==============================] - 27s 15ms/step - loss: 0.3813\n",
            "Epoch 93/100\n",
            "1835/1838 [============================>.] - ETA: 0s - loss: 0.3799\n",
            "Epoch 93: loss did not improve from 0.37932\n",
            "1838/1838 [==============================] - 27s 15ms/step - loss: 0.3801\n",
            "Epoch 94/100\n",
            "1835/1838 [============================>.] - ETA: 0s - loss: 0.3784\n",
            "Epoch 94: loss improved from 0.37932 to 0.37855, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 33s 18ms/step - loss: 0.3785\n",
            "Epoch 95/100\n",
            "1838/1838 [==============================] - ETA: 0s - loss: 0.3716\n",
            "Epoch 95: loss improved from 0.37855 to 0.37159, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 28s 15ms/step - loss: 0.3716\n",
            "Epoch 96/100\n",
            "1838/1838 [==============================] - ETA: 0s - loss: 0.3743\n",
            "Epoch 96: loss did not improve from 0.37159\n",
            "1838/1838 [==============================] - 28s 15ms/step - loss: 0.3743\n",
            "Epoch 97/100\n",
            "1838/1838 [==============================] - ETA: 0s - loss: 0.3780\n",
            "Epoch 97: loss did not improve from 0.37159\n",
            "1838/1838 [==============================] - 27s 15ms/step - loss: 0.3780\n",
            "Epoch 98/100\n",
            "1838/1838 [==============================] - ETA: 0s - loss: 0.3703\n",
            "Epoch 98: loss improved from 0.37159 to 0.37032, saving model to next_word.h5\n",
            "1838/1838 [==============================] - 36s 20ms/step - loss: 0.3703\n",
            "Epoch 99/100\n",
            "1837/1838 [============================>.] - ETA: 0s - loss: 0.3717\n",
            "Epoch 99: loss did not improve from 0.37032\n",
            "1838/1838 [==============================] - 27s 15ms/step - loss: 0.3717\n",
            "Epoch 100/100\n",
            "1838/1838 [==============================] - ETA: 0s - loss: 0.3741\n",
            "Epoch 100: loss did not improve from 0.37032\n",
            "1838/1838 [==============================] - 27s 15ms/step - loss: 0.3741\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2a904da500>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "ckpt = ModelCheckpoint(\"next_word.h5\", monitor = 'loss', verbose = 1, save_best_only = True)\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = Adam(learning_rate = 0.001))\n",
        "model.fit(X,y,epochs = 100,batch_size = 64,callbacks=[ckpt])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "tTb5p7WVTdYg"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "import pickle\n",
        "\n",
        "model = load_model('next_word.h5')\n",
        "tokenizer = pickle.load(open('token.pkl','rb'))\n",
        "\n",
        "def Predict_Next_Word(model,tokenizer,text):\n",
        "  sequence = tokenizer.texts_to_sequences([text])\n",
        "  sequence = np.array(sequence)\n",
        "  preds = np.argmax(model.predict(sequence))\n",
        "  predicted_word = \"\"\n",
        "\n",
        "  for key,val in tokenizer.word_index.items():\n",
        "    if val == preds:\n",
        "      predicted_word = key\n",
        "      break\n",
        "    \n",
        "  print(predicted_word)\n",
        "  return predicted_word"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# To Test the model\n",
        "  loss and accuracy are displayed above.\n",
        "  this model takes quiet a good computing power.\n",
        "  10 GB RAM\n",
        "  and 10 GB GPU RAM on colab.\n",
        "  <br><br>\n",
        "  TOOK a while to prepare a good model as i have low computing power \n",
        "  <br>\n",
        "  <br>\n",
        "\n",
        "<h2>\n",
        "<table>\n",
        "<tr>\n",
        "<td>Input</td>\n",
        "<td> Correct Output</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "<td></td>\n",
        "<td></td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "<td>i have a</td>\n",
        "<td> fury</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "<td>his nose was</td>\n",
        "<td>        small</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "<td>one friend and </td>\n",
        "<td>     protector</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "<td>in her with      </td>\n",
        "<td>   mournful</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "<td>shouts and stories  </td>\n",
        "<td> began </td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "<td>flew into a        </td>\n",
        "<td> passion</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "<td>and in its place had been put a large old</td>\n",
        "<td>mahagony</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "<td>Ivan's Nightmare I am not a</td>\n",
        "<td>                         doctor</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "<td>I am more afraid of her than of</td>\n",
        "<td>anything</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "<td>The Speech For The</td>\n",
        "<td>Defense</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "<td>We have heard the prosecutor himself admit that until to-day he   \n",
        "</td>\n",
        "<td>\n",
        "hesitated\n",
        "</td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "<td>to bind and to </td>\n",
        "<td>    loose</td>\n",
        "</tr>\n",
        "\n",
        "</table>\n",
        "</h2>\n",
        "\n",
        "\n",
        "\n",
        "For predicted output see the below cell"
      ],
      "metadata": {
        "id": "sBL7SJRG3DHo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7Yccq-MaXsbG",
        "outputId": "e5fe481e-f2f7-4240-c359-6c07ac635051"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your line : and in its place had been put a large old\n",
            "['a', 'large', 'old']\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "mahogany\n",
            "Ans :  mahogany\n",
            "Enter your line : Ivans Nightmare I am not a\n",
            "['am', 'not', 'a']\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "thief\n",
            "Ans :  thief\n",
            "Enter your line : I am more afraid of her than of\n",
            "['her', 'than', 'of']\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "anything\n",
            "Ans :  anything\n",
            "Enter your line :  The Speech For The\n",
            "['Speech', 'For', 'The']\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "defense\n",
            "Ans :  defense\n",
            "Enter your line : flew into a\n",
            "['flew', 'into', 'a']\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "fury\n",
            "Ans :  fury\n",
            "Enter your line : shouts and stories\n",
            "['shouts', 'and', 'stories']\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "began\n",
            "Ans :  began\n",
            "Enter your line : in her with\n",
            "['in', 'her', 'with']\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "mournful\n",
            "Ans :  mournful\n",
            "Enter your line : one friend and\n",
            "['one', 'friend', 'and']\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "protector\n",
            "Ans :  protector\n",
            "Enter your line : his nose was\n",
            "['his', 'nose', 'was']\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "small\n",
            "Ans :  small\n",
            "Enter your line : i have a\n",
            "['i', 'have', 'a']\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "positive\n",
            "Ans :  positive\n",
            "Enter your line : sherlock is a\n",
            "['sherlock', 'is', 'a']\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "charming\n",
            "Ans :  charming\n",
            "Enter your line : sherlock holmes is a wonderful\n",
            "['is', 'a', 'wonderful']\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "profit\n",
            "Ans :  profit\n",
            "Enter your line : sherlock holmes is a\n",
            "['holmes', 'is', 'a']\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "charming\n",
            "Ans :  charming\n",
            "Enter your line : We have heard the prosecutor himself admit that until today he\n",
            "['until', 'today', 'he']\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "hesitated\n",
            "Ans :  hesitated\n",
            "Enter your line :  to bind and to\n",
            "['bind', 'and', 'to']\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "loose\n",
            "Ans :  loose\n",
            "Enter your line : 0\n",
            "Stopped\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ntest lines \\ninput               output\\n\\ni have a            fury\\n\\nhis nose was        small\\n\\nhis one friend      and \\none friend and      protector\\nin her with         mournful\\nshouts and stories  began  \\nflew into a         passion\\n\\n\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "while(True):\n",
        "  text = input(\"Enter your line : \")\n",
        "\n",
        "  if text == \"0\":\n",
        "    print(\"Stopped\")\n",
        "    break\n",
        "  else:\n",
        "    try:\n",
        "      text = text.split(\" \")\n",
        "      text = text[-3:]\n",
        "\n",
        "      print(text)\n",
        "      ans = Predict_Next_Word(model,tokenizer,text)\n",
        "      print(\"Ans : \",ans)\n",
        "    except Exception as e:\n",
        "      print(\"Error : \",e)\n",
        "      continue\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "while(True):\n",
        "  text = input(\"Enter your line : \")\n",
        "\n",
        "  if text == \"stop\":\n",
        "    print(\"Stopped\")\n",
        "    break\n",
        "  else:\n",
        "    try:\n",
        "      text = text.split(\" \")\n",
        "      text = text[-3:]\n",
        "\n",
        "      print(text)\n",
        "      ans = Predict_Next_Word(model,tokenizer,text)\n",
        "      print(\"Ans : \",ans)\n",
        "    except Exception as e:\n",
        "      print(\"Error : \",e)\n",
        "      continue\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWcklfvW6_j4",
        "outputId": "80c03ba9-317e-41dc-c064-f80089ca3200"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your line : Nisarg is a\n",
            "['Nisarg', 'is', 'a']\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "charming\n",
            "Ans :  charming\n",
            "Enter your line : this boy is\n",
            "['this', 'boy', 'is']\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "called\n",
            "Ans :  called\n",
            "Enter your line : naruto is legend\n",
            "['naruto', 'is', 'legend']\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "i\n",
            "Ans :  i\n",
            "Enter your line : my self\n",
            "['my', 'self']\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "head\n",
            "Ans :  head\n",
            "Enter your line : why i love\n",
            "['why', 'i', 'love']\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "you\n",
            "Ans :  you\n",
            "Enter your line : this is why i love\n",
            "['why', 'i', 'love']\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "you\n",
            "Ans :  you\n",
            "Enter your line : hate is the reason\n",
            "['is', 'the', 'reason']\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "he\n",
            "Ans :  he\n",
            "Enter your line : he is the\n",
            "['he', 'is', 'the']\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "third\n",
            "Ans :  third\n",
            "Enter your line : detectives are the\n",
            "['detectives', 'are', 'the']\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "murderer\n",
            "Ans :  murderer\n",
            "Enter your line : criminals should be\n",
            "['criminals', 'should', 'be']\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "dissolved\n",
            "Ans :  dissolved\n",
            "Enter your line : this case is\n",
            "['this', 'case', 'is']\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "something\n",
            "Ans :  something\n",
            "Enter your line : on the third street i saw them \n",
            "['saw', 'them', '']\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "and\n",
            "Ans :  and\n",
            "Enter your line : 23 beaker street\n",
            "['23', 'beaker', 'street']\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "in\n",
            "Ans :  in\n",
            "Enter your line : this is my house you are\n",
            "['house', 'you', 'are']\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "not\n",
            "Ans :  not\n",
            "Enter your line : william sheakspear was a\n",
            "['sheakspear', 'was', 'a']\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "a\n",
            "Ans :  a\n",
            "Enter your line : stop\n",
            "Stopped\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JtjS0TSs9AzP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}