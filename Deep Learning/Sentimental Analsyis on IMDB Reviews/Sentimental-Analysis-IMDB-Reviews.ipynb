{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Sentimental Analysis on IMDB Reviews Dataset**\n### IMDB dataset having 50K movie reviews for natural language processing or Text analytics.\n#### This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training and 25,000 for testing. So, predict the number of positive and negative reviews using either classification or deep learning algorithms.\n\n### Dataset : https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews","metadata":{}},{"cell_type":"markdown","source":"## **Import required libraries**","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\nfrom keras.utils.np_utils import to_categorical\nfrom nltk.corpus import stopwords\nfrom nltk import word_tokenize\nimport re\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2023-07-25T08:02:40.949029Z","iopub.execute_input":"2023-07-25T08:02:40.949443Z","iopub.status.idle":"2023-07-25T08:02:52.908594Z","shell.execute_reply.started":"2023-07-25T08:02:40.949408Z","shell.execute_reply":"2023-07-25T08:02:52.907277Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## **Loading and preprocessing the data**","metadata":{}},{"cell_type":"code","source":"# Load the dataset\n# Please replace the path with the actual path to the IMDB dataset\ndata = pd.read_csv('/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\n\n# Text preprocessing\nSTOPWORDS = set(stopwords.words('english'))\ndef clean_text(text):\n    text = text.lower() # lowercase text\n    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwords from text\n    text = re.sub(r'\\W', ' ', text) # Remove all the special characters\n    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)  # remove all single characters\n    text = re.sub(r'\\^[a-zA-Z]\\s+', ' ', text) # Remove single characters from the start\n    text = re.sub(r'\\s+', ' ', text, flags=re.I) # Substituting multiple spaces with single space\n    return text\ndata['review'] = data['review'].apply(clean_text)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-25T08:03:55.376250Z","iopub.execute_input":"2023-07-25T08:03:55.377207Z","iopub.status.idle":"2023-07-25T08:04:14.947343Z","shell.execute_reply.started":"2023-07-25T08:03:55.377169Z","shell.execute_reply":"2023-07-25T08:04:14.946240Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## **Tokenization and Sequence Padding**","metadata":{}},{"cell_type":"code","source":"# The maximum number of words to be used. (most frequent)\nMAX_NB_WORDS = 50000\n# Max number of words in each complaint.\nMAX_SEQUENCE_LENGTH = 250\n# This is fixed.\nEMBEDDING_DIM = 100\n\ntokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\ntokenizer.fit_on_texts(data['review'].values)\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))\n\nX = tokenizer.texts_to_sequences(data['review'].values)\nX = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\nprint('Shape of data tensor:', X.shape)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-25T08:04:40.291393Z","iopub.execute_input":"2023-07-25T08:04:40.292578Z","iopub.status.idle":"2023-07-25T08:04:55.240876Z","shell.execute_reply.started":"2023-07-25T08:04:40.292540Z","shell.execute_reply":"2023-07-25T08:04:55.239686Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Found 101702 unique tokens.\nShape of data tensor: (50000, 250)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## **Splitting the data into training and testing data**","metadata":{}},{"cell_type":"code","source":"# Converting categorical labels to numbers.\nY = pd.get_dummies(data['sentiment']).values\nprint('Shape of label tensor:', Y.shape)\n\n# Train test split\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, random_state = 42)\nprint(X_train.shape,Y_train.shape)\nprint(X_test.shape,Y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2023-07-25T08:05:20.260739Z","iopub.execute_input":"2023-07-25T08:05:20.261185Z","iopub.status.idle":"2023-07-25T08:05:20.310899Z","shell.execute_reply.started":"2023-07-25T08:05:20.261152Z","shell.execute_reply":"2023-07-25T08:05:20.309657Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Shape of label tensor: (50000, 2)\n(40000, 250) (40000, 2)\n(10000, 250) (10000, 2)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## **Training the Model**","metadata":{}},{"cell_type":"code","source":"# Define LSTM model\nmodel = Sequential()\nmodel.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())\n\n# Train the model\nepochs = 5\nbatch_size = 64\n\nhistory = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_split=0.1)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-25T08:11:19.946819Z","iopub.execute_input":"2023-07-25T08:11:19.947366Z","iopub.status.idle":"2023-07-25T08:34:45.712861Z","shell.execute_reply.started":"2023-07-25T08:11:19.947333Z","shell.execute_reply":"2023-07-25T08:34:45.711387Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Model: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n embedding_1 (Embedding)     (None, 250, 100)          5000000   \n                                                                 \n spatial_dropout1d_1 (Spatia  (None, 250, 100)         0         \n lDropout1D)                                                     \n                                                                 \n lstm_1 (LSTM)               (None, 100)               80400     \n                                                                 \n dense_1 (Dense)             (None, 2)                 202       \n                                                                 \n=================================================================\nTotal params: 5,080,602\nTrainable params: 5,080,602\nNon-trainable params: 0\n_________________________________________________________________\nNone\nEpoch 1/5\n563/563 [==============================] - 271s 474ms/step - loss: 0.3519 - accuracy: 0.8445 - val_loss: 0.2738 - val_accuracy: 0.8892\nEpoch 2/5\n563/563 [==============================] - 270s 480ms/step - loss: 0.1694 - accuracy: 0.9373 - val_loss: 0.2714 - val_accuracy: 0.8892\nEpoch 3/5\n563/563 [==============================] - 271s 481ms/step - loss: 0.0934 - accuracy: 0.9686 - val_loss: 0.3123 - val_accuracy: 0.8898\nEpoch 4/5\n563/563 [==============================] - 270s 480ms/step - loss: 0.0614 - accuracy: 0.9793 - val_loss: 0.4298 - val_accuracy: 0.8823\nEpoch 5/5\n563/563 [==============================] - 271s 481ms/step - loss: 0.0456 - accuracy: 0.9847 - val_loss: 0.4704 - val_accuracy: 0.8798\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## **Evaluation of Model**","metadata":{}},{"cell_type":"code","source":"# Validate the model\naccr = model.evaluate(X_test,Y_test)\nprint('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n","metadata":{"execution":{"iopub.status.busy":"2023-07-25T08:35:30.995192Z","iopub.execute_input":"2023-07-25T08:35:30.995620Z","iopub.status.idle":"2023-07-25T08:35:47.764320Z","shell.execute_reply.started":"2023-07-25T08:35:30.995588Z","shell.execute_reply":"2023-07-25T08:35:47.762917Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"313/313 [==============================] - 17s 53ms/step - loss: 0.4584 - accuracy: 0.8778\nTest set\n  Loss: 0.458\n  Accuracy: 0.878\n","output_type":"stream"}]}]}