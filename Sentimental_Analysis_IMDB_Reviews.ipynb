{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Soumya1219/MindWave/blob/sample/Sentimental_Analysis_IMDB_Reviews.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Sentimental Analysis on IMDB Reviews Dataset**\n",
        "### IMDB dataset having 50K movie reviews for natural language processing or Text analytics.\n",
        "#### This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training and 25,000 for testing. So, predict the number of positive and negative reviews using either classification or deep learning algorithms.\n",
        "\n",
        "### Dataset : https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews"
      ],
      "metadata": {
        "id": "wLNmxXNkgDe7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Import required libraries**"
      ],
      "metadata": {
        "id": "LpFLn0Q6gDe9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "import re\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-25T08:02:40.949029Z",
          "iopub.execute_input": "2023-07-25T08:02:40.949443Z",
          "iopub.status.idle": "2023-07-25T08:02:52.908594Z",
          "shell.execute_reply.started": "2023-07-25T08:02:40.949408Z",
          "shell.execute_reply": "2023-07-25T08:02:52.907277Z"
        },
        "trusted": true,
        "id": "_r9c3S6JgDe-",
        "outputId": "08e82a17-b778-42a2-f132-47a6768987d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Loading and preprocessing the data**"
      ],
      "metadata": {
        "id": "_-EDHy2_gDfA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "# Please replace the path with the actual path to the IMDB dataset\n",
        "data = pd.read_csv('/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\n",
        "\n",
        "# Text preprocessing\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "def clean_text(text):\n",
        "    text = text.lower() # lowercase text\n",
        "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwords from text\n",
        "    text = re.sub(r'\\W', ' ', text) # Remove all the special characters\n",
        "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)  # remove all single characters\n",
        "    text = re.sub(r'\\^[a-zA-Z]\\s+', ' ', text) # Remove single characters from the start\n",
        "    text = re.sub(r'\\s+', ' ', text, flags=re.I) # Substituting multiple spaces with single space\n",
        "    return text\n",
        "data['review'] = data['review'].apply(clean_text)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-25T08:03:55.376250Z",
          "iopub.execute_input": "2023-07-25T08:03:55.377207Z",
          "iopub.status.idle": "2023-07-25T08:04:14.947343Z",
          "shell.execute_reply.started": "2023-07-25T08:03:55.377169Z",
          "shell.execute_reply": "2023-07-25T08:04:14.946240Z"
        },
        "trusted": true,
        "id": "6z9y3Ts7gDfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tokenization and Sequence Padding**"
      ],
      "metadata": {
        "id": "veb16mJggDfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The maximum number of words to be used. (most frequent)\n",
        "MAX_NB_WORDS = 50000\n",
        "# Max number of words in each complaint.\n",
        "MAX_SEQUENCE_LENGTH = 250\n",
        "# This is fixed.\n",
        "EMBEDDING_DIM = 100\n",
        "\n",
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
        "tokenizer.fit_on_texts(data['review'].values)\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "X = tokenizer.texts_to_sequences(data['review'].values)\n",
        "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "print('Shape of data tensor:', X.shape)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-25T08:04:40.291393Z",
          "iopub.execute_input": "2023-07-25T08:04:40.292578Z",
          "iopub.status.idle": "2023-07-25T08:04:55.240876Z",
          "shell.execute_reply.started": "2023-07-25T08:04:40.292540Z",
          "shell.execute_reply": "2023-07-25T08:04:55.239686Z"
        },
        "trusted": true,
        "id": "FhyvXpa1gDfB",
        "outputId": "df66b6e6-2e8e-46ad-e19f-181f9be1cbfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Found 101702 unique tokens.\nShape of data tensor: (50000, 250)\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Splitting the data into training and testing data**"
      ],
      "metadata": {
        "id": "GioVMtTjgDfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting categorical labels to numbers.\n",
        "Y = pd.get_dummies(data['sentiment']).values\n",
        "print('Shape of label tensor:', Y.shape)\n",
        "\n",
        "# Train test split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, random_state = 42)\n",
        "print(X_train.shape,Y_train.shape)\n",
        "print(X_test.shape,Y_test.shape)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-25T08:05:20.260739Z",
          "iopub.execute_input": "2023-07-25T08:05:20.261185Z",
          "iopub.status.idle": "2023-07-25T08:05:20.310899Z",
          "shell.execute_reply.started": "2023-07-25T08:05:20.261152Z",
          "shell.execute_reply": "2023-07-25T08:05:20.309657Z"
        },
        "trusted": true,
        "id": "XX00ujtPgDfC",
        "outputId": "507621d4-cdff-4eb5-81c1-df28138f4aea"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Shape of label tensor: (50000, 2)\n(40000, 250) (40000, 2)\n(10000, 250) (10000, 2)\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training the Model**"
      ],
      "metadata": {
        "id": "LOeoG31LgDfC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define LSTM model\n",
        "model = Sequential()\n",
        "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
        "model.add(SpatialDropout1D(0.2))\n",
        "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "\n",
        "# Train the model\n",
        "epochs = 5\n",
        "batch_size = 64\n",
        "\n",
        "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_split=0.1)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-25T08:11:19.946819Z",
          "iopub.execute_input": "2023-07-25T08:11:19.947366Z",
          "iopub.status.idle": "2023-07-25T08:34:45.712861Z",
          "shell.execute_reply.started": "2023-07-25T08:11:19.947333Z",
          "shell.execute_reply": "2023-07-25T08:34:45.711387Z"
        },
        "trusted": true,
        "id": "NU20Hqe7gDfC",
        "outputId": "749497b1-a6dc-4bd7-cd67-ae60836584e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Model: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n embedding_1 (Embedding)     (None, 250, 100)          5000000   \n                                                                 \n spatial_dropout1d_1 (Spatia  (None, 250, 100)         0         \n lDropout1D)                                                     \n                                                                 \n lstm_1 (LSTM)               (None, 100)               80400     \n                                                                 \n dense_1 (Dense)             (None, 2)                 202       \n                                                                 \n=================================================================\nTotal params: 5,080,602\nTrainable params: 5,080,602\nNon-trainable params: 0\n_________________________________________________________________\nNone\nEpoch 1/5\n563/563 [==============================] - 271s 474ms/step - loss: 0.3519 - accuracy: 0.8445 - val_loss: 0.2738 - val_accuracy: 0.8892\nEpoch 2/5\n563/563 [==============================] - 270s 480ms/step - loss: 0.1694 - accuracy: 0.9373 - val_loss: 0.2714 - val_accuracy: 0.8892\nEpoch 3/5\n563/563 [==============================] - 271s 481ms/step - loss: 0.0934 - accuracy: 0.9686 - val_loss: 0.3123 - val_accuracy: 0.8898\nEpoch 4/5\n563/563 [==============================] - 270s 480ms/step - loss: 0.0614 - accuracy: 0.9793 - val_loss: 0.4298 - val_accuracy: 0.8823\nEpoch 5/5\n563/563 [==============================] - 271s 481ms/step - loss: 0.0456 - accuracy: 0.9847 - val_loss: 0.4704 - val_accuracy: 0.8798\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Evaluation of Model**"
      ],
      "metadata": {
        "id": "q5ZrgpqcgDfD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Validate the model\n",
        "accr = model.evaluate(X_test,Y_test)\n",
        "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-25T08:35:30.995192Z",
          "iopub.execute_input": "2023-07-25T08:35:30.995620Z",
          "iopub.status.idle": "2023-07-25T08:35:47.764320Z",
          "shell.execute_reply.started": "2023-07-25T08:35:30.995588Z",
          "shell.execute_reply": "2023-07-25T08:35:47.762917Z"
        },
        "trusted": true,
        "id": "D4m5-KsygDfD",
        "outputId": "8d720948-e1f2-47c7-9e8b-ecb8bee04226"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "313/313 [==============================] - 17s 53ms/step - loss: 0.4584 - accuracy: 0.8778\nTest set\n  Loss: 0.458\n  Accuracy: 0.878\n",
          "output_type": "stream"
        }
      ]
    }
  ]
}